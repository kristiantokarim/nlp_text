{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import numpy\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Dropout\n",
    "import string\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from keras import optimizers\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sentence(sentence):\n",
    "\t#sentence = string.decode('utf-8')\n",
    "\tsentence = (str(sentence)).lower()\n",
    "\n",
    "\t# unit\n",
    "\t#weight\n",
    "\tsentence = re.sub(r\"(\\d+)kgs \", lambda m: m.group(1) + ' kg ', sentence)\t# e.g. 4kgs => 4 kg\n",
    "\tsentence = re.sub(r\"(\\d+)kg \", lambda m: m.group(1) + ' kg ', sentence)\t # e.g. 4kg => 4 kg\n",
    "\tsentence = re.sub(r\"(\\d+)k \", lambda m: m.group(1) + '000 ', sentence)\t  # e.g. 4k => 4000\n",
    "\t#lenght\n",
    "\tsentence = re.sub(r\"(\\d+)km \", lambda m: m.group(1) + ' km ', sentence)\n",
    "\tsentence = re.sub(r\"(\\d+)cm \", lambda m: m.group(1) + ' cm ', sentence)\n",
    "\tsentence = re.sub(r\"(\\d+)m \", lambda m: m.group(1) + ' m ', sentence)\n",
    "\t#data\n",
    "\tsentence = re.sub(r\"(\\d+)gb \", lambda m: m.group(1) + ' gb ', sentence)\n",
    "\t#money\n",
    "\tsentence = re.sub(r\"\\$(\\d+)\", lambda m: m.group(1) + ' dollar ', sentence)\n",
    "\tsentence = re.sub(r\"(\\d+)\\$\", lambda m: m.group(1) + ' dollar ', sentence)\n",
    "\n",
    "\t# acronym\n",
    "\tsentence = re.sub(r\"can\\'t\", \"can not\", sentence)\n",
    "\tsentence = re.sub(r\"cannot\", \"can not \", sentence)\n",
    "\tsentence = re.sub(r\"what\\'s\", \"what is\", sentence)\n",
    "\tsentence = re.sub(r\"\\'ve \", \" have \", sentence)\n",
    "\tsentence = re.sub(r\"n\\'t\", \" not \", sentence)\n",
    "\tsentence = re.sub(r\"i\\'m\", \"i am \", sentence)\n",
    "\tsentence = re.sub(r\"\\'re\", \" are \", sentence)\n",
    "\tsentence = re.sub(r\"\\'d\", \" would \", sentence)\n",
    "\tsentence = re.sub(r\"\\'ll\", \" will \", sentence)\n",
    "\tsentence = re.sub(r\" e mail \", \" email \", sentence)\n",
    "\tsentence = re.sub(r\" e \\- mail \", \" email \", sentence)\n",
    "\tsentence = re.sub(r\"e-mail\", \" email \", sentence)\n",
    "\tsentence = re.sub(r\",000\", '000', sentence)\n",
    "\tsentence = re.sub(r\"\\'s\", \" \", sentence)\n",
    "\t\n",
    "\t# punctuation\n",
    "\tsentence = re.sub(r\"\\+\", \" + \", sentence)\n",
    "\tsentence = re.sub(r\"'\", \" \", sentence)\n",
    "\tsentence = re.sub(r\"\\\"\", \"\", sentence)\n",
    "\tsentence = re.sub(r\"-\", \" \", sentence)\n",
    "\tsentence = re.sub(r\"/\", \" / \", sentence)\n",
    "\tsentence = re.sub(r\"\\\\\", \" \\ \", sentence)\n",
    "\tsentence = re.sub(r\"=\", \" = \", sentence)\n",
    "\tsentence = re.sub(r\"\\^\", \" ^ \", sentence)\n",
    "\tsentence = re.sub(r\":\", \" : \", sentence)\n",
    "\tsentence = re.sub(r\"\\.\", \"\", sentence)\n",
    "\tsentence = re.sub(r\",\", \" \", sentence)\n",
    "\tsentence = re.sub(r\"\\?\", \"\", sentence)\n",
    "\tsentence = re.sub(r\"!\", \"\", sentence)\n",
    "\tsentence = re.sub(r\"\\\"\", \" \\\" \", sentence)\n",
    "\tsentence = re.sub(r\"&\", \" & \", sentence)\n",
    "\tsentence = re.sub(r\"\\|\", \" | \", sentence)\n",
    "\tsentence = re.sub(r\";\", \" ; \", sentence)\n",
    "\tsentence = re.sub(r\"\\(\", \" ( \", sentence)\n",
    "\tsentence = re.sub(r\"\\)\", \" ) \", sentence)\n",
    "\n",
    "\t# symbol replacement\n",
    "\tsentence = re.sub(r\"&\", \" and \", sentence)\n",
    "\tsentence = re.sub(r\"\\|\", \" or \", sentence)\n",
    "\tsentence = re.sub(r\"\\\\\", \" or \", sentence)\n",
    "\tsentence = re.sub(r\"/\", \" or \", sentence)\n",
    "\tsentence = re.sub(r\"=\", \" equal \", sentence)\n",
    "\tsentence = re.sub(r\"\\+\", \" plus \", sentence)\n",
    "\tsentence = re.sub(r\"â‚¹\", \" rs \", sentence)\n",
    "\tsentence = re.sub(r\"\\$\", \" dollar \", sentence)\n",
    "\tsentence = re.sub(r\"%\", \" percent \", sentence)\n",
    "\n",
    "\twhile '  ' in sentence:\n",
    "\t\tsentence = re.sub(r\"  \", \" \", sentence)\n",
    "\treturn sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset = pd.read_csv('train.csv').values[:,3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset = numpy.asanyarray(sorted(total_dataset,key=lambda x: x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300-SLIM.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_positive = []\n",
    "train_negative = []\n",
    "test_positive = []\n",
    "test_negative = []\n",
    "i = 0\n",
    "j = len(total_dataset) - 1\n",
    "while (i < j):\n",
    "    if total_dataset[i][2] == 0:\n",
    "        if len(test_negative) < len(train_negative):\n",
    "            test_negative.append(total_dataset[i])\n",
    "        else:\n",
    "            train_negative.append(total_dataset[i])\n",
    "    if total_dataset[j][2] == 1:\n",
    "        if len(test_positive) < len(train_positive):\n",
    "            test_positive.append(total_dataset[j])\n",
    "        else:\n",
    "            train_positive.append(total_dataset[j])\n",
    "    j = j - 1\n",
    "    i = i + 1              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_positive = train_positive[0:20000]\n",
    "train_negative = train_negative[0:20000]\n",
    "total_dataset = train_positive\n",
    "total_dataset.extend(train_negative)\n",
    "total_dataset = numpy.asanyarray(total_dataset)\n",
    "test_positive = test_positive[0:20000]\n",
    "test_negative = test_negative[0:20000]\n",
    "total_dataset_test = test_positive\n",
    "total_dataset_test.extend(test_negative)\n",
    "total_dataset_test = numpy.asanyarray(total_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sentence_list(list_sentence):\n",
    "\tfor i in range (0,len(list_sentence)):\n",
    "\t\tlist_sentence[i] = normalize_sentence(list_sentence[i])\n",
    "\treturn list_sentence\n",
    "\n",
    "def normalize_sentence_dataset(dataset):\n",
    "\tdataset[:,0] = normalize_sentence_list(dataset[:,0])\n",
    "\tdataset[:,1] = normalize_sentence_list(dataset[:,1])\n",
    "\treturn dataset\n",
    "\n",
    "def lemmatize_sentence_dataset(dataset):\n",
    "\tlemmatizer = WordNetLemmatizer()\n",
    "\tfor idx_col in range(0,2):\n",
    "\t\tfor idx_row in range (0,len(dataset[:,0])):  \n",
    "\t\t\ttoken_list = dataset[idx_row,idx_col].split(\" \")\n",
    "\t\t\tdataset[idx_row,idx_col] = \"\"\n",
    "\t\t\tfor i in range(0,len(token_list)):\n",
    "\t\t\t\tdataset[idx_row,idx_col] += lemmatizer.lemmatize(token_list[i])+\" \"\n",
    "\treturn dataset\n",
    "\n",
    "def remove_stop_word(sentence):\n",
    "\tfor stopword in stopwords.words('english'):\n",
    "\t\t\tsentence = re.sub(r\" \"+stopword+\" \", \" \", sentence)\n",
    "\treturn sentence\n",
    "def get_avg_vector(sentence, model, num_features=300):\n",
    "\tsentence = sentence.strip().split(' ')\n",
    "\tsentence_avg_vec = numpy.zeros((num_features,),dtype=\"float32\")\n",
    "\tn_vocab_word = 0\n",
    "\tfor word in sentence:\n",
    "\t\tif (word in model.wv.vocab):\n",
    "\t\t\tn_vocab_word += 1\n",
    "\t\t\tsentence_avg_vec = numpy.add(sentence_avg_vec, model[word])\n",
    "\t\telif (word.title() in model.wv.vocab):\n",
    "\t\t\tn_vocab_word += 1\n",
    "\t\t\tsentence_avg_vec = numpy.add(sentence_avg_vec, model[word.title()])\n",
    "\t\telif (word.upper() in model.wv.vocab):\n",
    "\t\t\tn_vocab_word += 1\n",
    "\t\t\tsentence_avg_vec = numpy.add(sentence_avg_vec, model[word.upper()])\n",
    "\tif (n_vocab_word != 0):\n",
    "\t\tsentence_avg_vec = numpy.divide(sentence_avg_vec, n_vocab_word)\n",
    "\treturn sentence_avg_vec\n",
    "\n",
    "def get_number_count(sentence):\n",
    "\tcount = 0\n",
    "\tfor word in sentence.strip().split(' '):\n",
    "\t\tif (word.isdigit()):\n",
    "\t\t\tcount += 1\n",
    "\treturn count\n",
    "\t\n",
    "def get_oov_count(sentence, model):\n",
    "\tcount = 0\n",
    "\tsentence = sentence.strip().split(' ')\n",
    "\tfor word in sentence:\n",
    "\t\tif ((word not in model.wv.vocab) and (word.title() in model.wv.vocab) and (word.upper() in model.wv.vocab)):\n",
    "\t\t\tcount += 1\n",
    "\treturn count\n",
    "\t\n",
    "def get_features(data, model):\n",
    "\tq1 = get_avg_vector(data[0], model)\n",
    "\tq2 = get_avg_vector(data[1], model)\n",
    "\tn_w_q1 = len(data[0].strip().split(' '))\n",
    "\tn_w_q2 = len(data[1].strip().split(' '))\n",
    "\tlist_features = []\n",
    "\tlist_features.extend(q1)\n",
    "\tlist_features.extend(q2)\n",
    "\tlist_features.append(numpy.dot(q1, q2)/(numpy.linalg.norm(q1)* numpy.linalg.norm(q2)))\n",
    "\tlist_features.append(n_w_q1)\n",
    "\tlist_features.append(n_w_q2)\n",
    "\tlist_features.extend([get_oov_count(data[1], model),get_oov_count(data[0], model),get_number_count(data[1]), abs(n_w_q1-n_w_q2)])\n",
    "\tlist_features.extend(get_avg_vector(data[0].split(' ')[0], model))\n",
    "\tlist_features.extend(get_avg_vector(data[1].split(' ')[0], model))\n",
    "\treturn list_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset = normalize_sentence_dataset(total_dataset)\n",
    "total_dataset_test = normalize_sentence_dataset(total_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset = lemmatize_sentence_dataset(total_dataset)\n",
    "total_dataset_test = lemmatize_sentence_dataset(total_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in total_dataset:\n",
    "    data[0] = remove_stop_word(data[0])\n",
    "    data[1] = remove_stop_word(data[1])\n",
    "    \n",
    "for data in total_dataset_test:\n",
    "    data[0] = remove_stop_word(data[0])\n",
    "    data[1] = remove_stop_word(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = []\n",
    "for data in total_dataset:\n",
    "    if data[0] is not None and data[1] is not None:\n",
    "        if all(c in string.printable for c in data[0]) and all(c in string.printable for c in data[1]):\n",
    "            temp_data.append(data)\n",
    "total_dataset = numpy.asanyarray(temp_data)\n",
    "temp_data = []\n",
    "for data in total_dataset_test:\n",
    "    if data[0] is not None and data[1] is not None:\n",
    "        if all(c in string.printable for c in data[0]) and all(c in string.printable for c in data[1]):\n",
    "            temp_data.append(data)\n",
    "total_dataset_test = numpy.asanyarray(temp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: RuntimeWarning: invalid value encountered in float_scalars\n"
     ]
    }
   ],
   "source": [
    "temp_data = []\n",
    "for data in total_dataset:\n",
    "    temp_data.append([get_features(data, word2vec), data[2]])\n",
    "total_dataset = numpy.asanyarray(temp_data)\n",
    "temp_data = []\n",
    "for data in total_dataset_test:\n",
    "    temp_data.append([get_features(data, word2vec), data[2]])\n",
    "total_dataset_test = numpy.asanyarray(temp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [x[0] for x in total_dataset]\n",
    "train_y = [x[1] for x in total_dataset]\n",
    "test_x = [x[0] for x in total_dataset_test]\n",
    "test_y = [x[1] for x in total_dataset_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Embedding(len(train_x[0]), output_dim=100))\n",
    "# model.add(LSTM(50))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
    "# model.fit(train_x, train_y, epochs=50000, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "train_xx = []\n",
    "train_yy = []\n",
    "for i,a in enumerate(train_x):\n",
    "    if all(not math.isnan(c) and not math.isinf(c) for c in train_x[i]):\n",
    "        train_xx.append(train_x[i])\n",
    "        train_yy.append(train_y[i])\n",
    "test_xx = []\n",
    "test_yy = []\n",
    "for i,a in enumerate(test_x):\n",
    "    if all(not math.isnan(c) and not math.isinf(c) for c in test_x[i]):\n",
    "        test_xx.append(test_x[i])\n",
    "        test_yy.append(test_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "scaler.fit(train_xx)\n",
    "train_xx = scaler.transform(train_xx)\n",
    "test_xx = scaler.transform(test_xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31351 samples, validate on 7838 samples\n",
      "Epoch 1/200\n",
      "31351/31351 [==============================] - 11s 342us/step - loss: 0.6640 - acc: 0.6174 - val_loss: 0.9370 - val_acc: 0.1180\n",
      "Epoch 2/200\n",
      "31351/31351 [==============================] - 10s 306us/step - loss: 0.6441 - acc: 0.6478 - val_loss: 0.9073 - val_acc: 0.1951\n",
      "Epoch 3/200\n",
      "31351/31351 [==============================] - 9s 298us/step - loss: 0.6341 - acc: 0.6598 - val_loss: 0.9254 - val_acc: 0.2057\n",
      "Epoch 4/200\n",
      "31351/31351 [==============================] - 9s 276us/step - loss: 0.6305 - acc: 0.6633 - val_loss: 0.9277 - val_acc: 0.2177\n",
      "Epoch 5/200\n",
      "31351/31351 [==============================] - 10s 317us/step - loss: 0.6261 - acc: 0.6687 - val_loss: 0.9147 - val_acc: 0.2383\n",
      "Epoch 6/200\n",
      "31351/31351 [==============================] - 9s 290us/step - loss: 0.6240 - acc: 0.6711 - val_loss: 0.9094 - val_acc: 0.2450\n",
      "Epoch 7/200\n",
      "31351/31351 [==============================] - 9s 291us/step - loss: 0.6223 - acc: 0.6702 - val_loss: 0.9045 - val_acc: 0.2557\n",
      "Epoch 8/200\n",
      "31351/31351 [==============================] - 9s 295us/step - loss: 0.6196 - acc: 0.6754 - val_loss: 0.8961 - val_acc: 0.2605\n",
      "Epoch 9/200\n",
      "31351/31351 [==============================] - 9s 291us/step - loss: 0.6167 - acc: 0.6736 - val_loss: 0.8974 - val_acc: 0.2672\n",
      "Epoch 10/200\n",
      "31351/31351 [==============================] - 9s 280us/step - loss: 0.6161 - acc: 0.6755 - val_loss: 0.8911 - val_acc: 0.2724\n",
      "Epoch 11/200\n",
      "31351/31351 [==============================] - 9s 275us/step - loss: 0.6164 - acc: 0.6762 - val_loss: 0.8923 - val_acc: 0.2730\n",
      "Epoch 12/200\n",
      "31351/31351 [==============================] - 9s 299us/step - loss: 0.6125 - acc: 0.6780 - val_loss: 0.8985 - val_acc: 0.2715\n",
      "Epoch 13/200\n",
      "31351/31351 [==============================] - 10s 304us/step - loss: 0.6116 - acc: 0.6768 - val_loss: 0.9005 - val_acc: 0.2726\n",
      "Epoch 14/200\n",
      "31351/31351 [==============================] - 10s 311us/step - loss: 0.6116 - acc: 0.6788 - val_loss: 0.9003 - val_acc: 0.2756\n",
      "Epoch 15/200\n",
      "31351/31351 [==============================] - 9s 296us/step - loss: 0.6091 - acc: 0.6790 - val_loss: 0.8950 - val_acc: 0.2823\n",
      "Epoch 16/200\n",
      "31351/31351 [==============================] - 9s 299us/step - loss: 0.6080 - acc: 0.6818 - val_loss: 0.8897 - val_acc: 0.2899\n",
      "Epoch 17/200\n",
      "31351/31351 [==============================] - 9s 284us/step - loss: 0.6073 - acc: 0.6803 - val_loss: 0.8986 - val_acc: 0.2848\n",
      "Epoch 18/200\n",
      "31351/31351 [==============================] - 9s 293us/step - loss: 0.6038 - acc: 0.6844 - val_loss: 0.8923 - val_acc: 0.2914\n",
      "Epoch 19/200\n",
      "31351/31351 [==============================] - 9s 291us/step - loss: 0.6033 - acc: 0.6843 - val_loss: 0.8786 - val_acc: 0.3044\n",
      "Epoch 20/200\n",
      "31351/31351 [==============================] - 9s 290us/step - loss: 0.6030 - acc: 0.6852 - val_loss: 0.8901 - val_acc: 0.2965\n",
      "Epoch 21/200\n",
      "31351/31351 [==============================] - 9s 289us/step - loss: 0.6015 - acc: 0.6858 - val_loss: 0.8967 - val_acc: 0.2947\n",
      "Epoch 22/200\n",
      "31351/31351 [==============================] - 9s 285us/step - loss: 0.5997 - acc: 0.6878 - val_loss: 0.8854 - val_acc: 0.3059\n",
      "Epoch 23/200\n",
      "31351/31351 [==============================] - 9s 285us/step - loss: 0.6004 - acc: 0.6890 - val_loss: 0.8878 - val_acc: 0.3089\n",
      "Epoch 24/200\n",
      "31351/31351 [==============================] - 9s 301us/step - loss: 0.5973 - acc: 0.6888 - val_loss: 0.8768 - val_acc: 0.3205\n",
      "Epoch 25/200\n",
      "31351/31351 [==============================] - 9s 300us/step - loss: 0.5970 - acc: 0.6909 - val_loss: 0.8987 - val_acc: 0.3099\n",
      "Epoch 26/200\n",
      "31351/31351 [==============================] - 9s 298us/step - loss: 0.5954 - acc: 0.6912 - val_loss: 0.8888 - val_acc: 0.3178\n",
      "Epoch 27/200\n",
      "31351/31351 [==============================] - 10s 304us/step - loss: 0.5951 - acc: 0.6892 - val_loss: 0.8808 - val_acc: 0.3242\n",
      "Epoch 28/200\n",
      "31351/31351 [==============================] - 9s 299us/step - loss: 0.5921 - acc: 0.6932 - val_loss: 0.8885 - val_acc: 0.3223\n",
      "Epoch 29/200\n",
      "31351/31351 [==============================] - 10s 305us/step - loss: 0.5907 - acc: 0.6933 - val_loss: 0.8871 - val_acc: 0.3234\n",
      "Epoch 30/200\n",
      "31351/31351 [==============================] - 10s 314us/step - loss: 0.5911 - acc: 0.6952 - val_loss: 0.8779 - val_acc: 0.3343\n",
      "Epoch 31/200\n",
      "31351/31351 [==============================] - 10s 306us/step - loss: 0.5886 - acc: 0.6968 - val_loss: 0.8790 - val_acc: 0.3367\n",
      "Epoch 32/200\n",
      "31351/31351 [==============================] - 9s 295us/step - loss: 0.5884 - acc: 0.6984 - val_loss: 0.8831 - val_acc: 0.3373\n",
      "Epoch 33/200\n",
      "31351/31351 [==============================] - 10s 321us/step - loss: 0.5890 - acc: 0.6979 - val_loss: 0.8868 - val_acc: 0.3349\n",
      "Epoch 34/200\n",
      "31351/31351 [==============================] - 9s 288us/step - loss: 0.5844 - acc: 0.7014 - val_loss: 0.8804 - val_acc: 0.3480\n",
      "Epoch 35/200\n",
      "31351/31351 [==============================] - 9s 293us/step - loss: 0.5846 - acc: 0.7007 - val_loss: 0.8702 - val_acc: 0.3580\n",
      "Epoch 36/200\n",
      "31351/31351 [==============================] - 9s 291us/step - loss: 0.5837 - acc: 0.7027 - val_loss: 0.8787 - val_acc: 0.3537\n",
      "Epoch 37/200\n",
      "31351/31351 [==============================] - 10s 310us/step - loss: 0.5825 - acc: 0.7045 - val_loss: 0.8683 - val_acc: 0.3635\n",
      "Epoch 38/200\n",
      "31351/31351 [==============================] - 11s 358us/step - loss: 0.5834 - acc: 0.7039 - val_loss: 0.8775 - val_acc: 0.3570\n",
      "Epoch 39/200\n",
      "31351/31351 [==============================] - 11s 353us/step - loss: 0.5806 - acc: 0.7055 - val_loss: 0.8737 - val_acc: 0.3655\n",
      "Epoch 40/200\n",
      "31351/31351 [==============================] - 11s 341us/step - loss: 0.5807 - acc: 0.7049 - val_loss: 0.8726 - val_acc: 0.3671\n",
      "Epoch 41/200\n",
      "31351/31351 [==============================] - 10s 318us/step - loss: 0.5793 - acc: 0.7078 - val_loss: 0.8646 - val_acc: 0.3752\n",
      "Epoch 42/200\n",
      "31264/31351 [============================>.] - ETA: 0s - loss: 0.5786 - acc: 0.7084"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(input_dim=len(train_xx[0]), units=150, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.Adagrad(lr=0.001), metrics=['accuracy'])\n",
    "model.fit(train_xx, train_yy, epochs=200, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19628/19628 [==============================] - 2s 106us/step\n"
     ]
    }
   ],
   "source": [
    "pred_y = model.predict_classes(test_xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = svm.SVC(verbose=10)\n",
    "cls.fit(numpy.asanyarray(train_xx), numpy.asanyarray(train_yy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = cls.predict(test_xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72507827307976369"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(pred_y, test_yy, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "`save_weights` requires h5py.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-b17d7b51bcf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_architecture.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# saving weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_weights.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/models.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite)\u001b[0m\n\u001b[1;32m    737\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`save_weights` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m         \u001b[0;31m# If file exists and should not be overwritten:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverwrite\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: `save_weights` requires h5py."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
